# Անկյունագծայնացում և SVD
- [Անկյունագծայնացում և SVD](#անկյունագծայնացում-և-svd)
  - [1. Մատրիցների անկյունագծայնացում](#1-մատրիցների-անկյունագծայնացում)
  - [2. Մատրիցի ռանգ (Rank)](#2-մատրիցի-ռանգ-rank)
  - [3. Եզակի արժեքների վերլուծություն (SVD)](#3-եզակի-արժեքների-վերլուծություն-svd)
  - [4. Գործնական օրինակների լուծում](#4-գործնական-օրինակների-լուծում)
  - [5. Կիրառություն. Կեղծ հակադարձ մատրից (Pseudo-inverse)](#5-կիրառություն-կեղծ-հակադարձ-մատրից-pseudo-inverse)
  - [Մուր-Պենրոուզի կեղծ հակադարձ մատրից (A^+)](#մուր-պենրոուզի-կեղծ-հակադարձ-մատրից-a)
  - [Կիրառություն. Լավագույն մոտարկման խնդիրը](#կիրառություն-լավագույն-մոտարկման-խնդիրը)
  - [Գործնական օրինակի լրիվ լուծում](#գործնական-օրինակի-լրիվ-լուծում)


## 1. Մատրիցների անկյունագծայնացում

**1.1. Սահմանում**
$A$ քառակուսի մատրիցը կոչվում է **անկյունագծայնացվող**, եթե այն նման է որևէ անկյունագծային $D$ մատրիցի:

$$A = P D P^{-1}$$

որտեղ $D$-ն անկյունագծային մատրից է, իսկ $P$-ն՝ հակադարձելի մատրից:

**1.2. Մատրիցի աստիճանի հաշվում**

Անկյունագծայնացումը թույլ է տալիս հեշտությամբ հաշվել մատրիցի բարձր աստիճանները:

$$A^2 = (P D P^{-1})(P D P^{-1}) = P D (P^{-1} P) D P^{-1} = P D^2 P^{-1}$$

Ընդհանուր դեպքում՝ **$A^n = P D^n P^{-1}$**:

**1.3. Սիմետրիկ մատրիցներ**

Եթե $A$-ն սիմետրիկ է ($A = A^T$), ապա այն անկյունագծայնացվում է **օրթոգոնալ** $P$ մատրիցի միջոցով:

Քանի որ օրթոգոնալ մատրիցի համար $P^{-1} = P^T$, ապա՝

**$A = P D P^T$**

**1.4. Հակադարձ մատրիցի սեփական արժեքները**

Եթե $Ax = \lambda x$ և $\lambda \neq 0$, ապա $A^{-1}$ մատրիցի սեփական արժեքը $1/\lambda$ է:

**Ապացույց.**

$$A x = \lambda x \implies A^{-1} (A x) = A^{-1} (\lambda x) \implies I x = \lambda A^{-1} x \implies A^{-1} x = \frac{1}{\lambda} x$$

---

## 2. Մատրիցի ռանգ (Rank)

**2.1. Սահմանում**

$A \in M_{m \times n}$ մատրիցի **տողային ռանգ** է կոչվում այդ մատրիցի առավելագույն քանակով գծորեն անկախ տողերի քանակը:

**2.2. Հիմնական թեորեմ**

Կամայական մատրիցի ռանգն ըստ տողերի հավասար է այդ մատրիցի ռանգին ըստ սյուների:

**$\text{rank}_{row}(A) = \text{rank}_{col}(A) = \text{rank}(A)$**

Նաև՝ $\text{rank}(A) = \text{rank}(A^T)$:

---

## 3. Եզակի արժեքների վերլուծություն (SVD)

**3.1. Թեորեմ**

Կամայական $A \in M_{m \times n}$ մատրիցի համար գոյություն ունեն $U \in M_{m \times m}$ և $V \in M_{n \times n}$ օրթոգոնալ մատրիցներ և $\Sigma \in M_{m \times n}$ անկյունագծային տեսքի մատրից այնպես, որ՝

**$A = U \Sigma V^T$**

- $U$-ի սյուները $A A^T$ մատրիցի սեփական վեկտորներն են:
    
- $V$-ի սյուները $A^T A$ մատրիցի սեփական վեկտորներն են:
    
- $\Sigma$-ի անկյունագծային տարրերը ($\sigma_i$) կոչվում են **եզակի արժեքներ** և $\sigma_i = \sqrt{\lambda_i}$, որտեղ $\lambda_i$-ն $A^T A$-ի սեփական արժեքներն են:
    

**3.2. Երկրաչափական մեկնաբանություն**

SVD-ն ներկայացնում է գծային ձևափոխությունը որպես երեք քայլ՝ պտույտ ($V^T$), ձգում ($\Sigma$) և նորից պտույտ ($U$): Միավոր շրջանագիծը վերածվում է էլիպսի, որի առանցքները $\sigma_i u_i$ վեկտորներն են:

---

## 4. Գործնական օրինակների լուծում

**Խնդիր 256.** $A^{10}$-ի հաշվում

Տրված է $A = \begin{pmatrix} 1 & 0 \\ -1 & 2 \end{pmatrix}$:

1. **Սեփական արժեքներ.** $\det(A-\lambda I) = (1-\lambda)(2-\lambda) = 0 \implies \lambda_1 = 1, \lambda_2 = 2$:
    
2. **Անկյունագծայնացում.** $A = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix}$:
    
3. **Աստիճան.** $A^{10} = P D^{10} P^{-1} = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} 1^{10} & 0 \\ 0 & 2^{10} \end{pmatrix} \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ -1023 & 1024 \end{pmatrix}$:
    

**Խնդիր**. $A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix}$ մատրիցի SVD վերլուծությունը

1. **Հաշվում ենք $A^T A$.**
    
    $A^T A = \begin{pmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$
    
2. **Սեփական արժեքներ.** $\lambda_1 = 3, \lambda_2 = 1$:
    
3. **Եզակի արժեքներ.** $\sigma_1 = \sqrt{3}, \sigma_2 = 1$:
    
4. **$V$ մատրից (սեփական վեկտորներ).** $v_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}, v_2 = \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix}$:
    
5. **$U$ մատրից.** Հաշվվում է $A A^T$-ի սեփական վեկտորների միջոցով:
    
    $U = \begin{pmatrix} 2/\sqrt{6} & 0 & -1/\sqrt{3} \\ 1/\sqrt{6} & 1/\sqrt{2} & 1/\sqrt{3} \\ 1/\sqrt{6} & -1/\sqrt{2} & 1/\sqrt{3} \end{pmatrix}$
    
6. **Վերջնական տեսք.** $A = U \Sigma V^T$:
    

---

## 5. Կիրառություն. Կեղծ հակադարձ մատրից (Pseudo-inverse)

Եթե $Ax = b$ համակարգը լուծում չունի, փնտրում ենք այնպիսի $x_0$, որ $\|Ax_0 - b\|$ լինի մինիմալ:

Լուծումն է՝ **$x_0 = A^+ b$**, որտեղ $A^+$ կոչվում է Մուր-Պենրոուզի կեղծ հակադարձ մատրից:


## Մուր-Պենրոուզի կեղծ հակադարձ մատրից (A^+)

Երբ մատրիցը քառակուսի չէ կամ հակադարձելի չէ, մենք օգտագործում ենք **կեղծ հակադարձ մատրիցը**։

Եթե $A \in M_{m \times n}$, ապա $A^+ \in M_{n \times m}$։

**Հաշվարկման բանաձևը SVD-ի միջոցով**

Եթե հայտնի է մատրիցի SVD վերլուծությունը՝ $A = U \Sigma V^T$, ապա կեղծ հակադարձը հաշվվում է հետևյալ կերպ.

**$A^+ = V \Sigma^+ U^T$**

որտեղ $\Sigma^+$-ը ստացվում է $\Sigma$-ից՝ անկյունագծային տարրերը փոխարինելով իրենց հակադարձներով ($1/\sigma_i$), իսկ մատրիցը տրանսպոզացվում է։

**Կեղծ հակադարձի հատկությունները (Պենրոուզի պայմաններ)**

$A^+$ մատրիցը միակն է, որը բավարարում է հետևյալ 4 պայմաններին․

1. $A A^+ A = A$
    
2. $A^+ A A^+ = A^+$
    
3. $(A A^+)^T = A A^+$ (սիմետրիկություն)
    
4. $(A^+ A)^T = A^+ A$ (սիմետրիկություն)
    

---

## Կիրառություն. Լավագույն մոտարկման խնդիրը

Եթե $Ax = b$ համակարգը չունի ճշգրիտ լուծում (օրինակ՝ երբ հավասարումների քանակը շատ է անհայտներից), մենք փնտրում ենք այնպիսի $x_0$ վեկտոր, որը **նվազագույնի է հասցնում սխալը**։

$$\|Ax_0 - b\| \to \min$$

Այս լավագույն մոտարկումը տրվում է հետևյալ բանաձևով․

**$x_0 = A^+ b$**

---

## Գործնական օրինակի լրիվ լուծում

**Տրված է.** $A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix}$ և $b = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$։

Քայլ 1. Գտնել SVD բաղադրիչները

- **Սեփական արժեքներ ($A^T A$).** $\lambda_1 = 3, \lambda_2 = 1$։
    
- **Եզակի արժեքներ.** $\sigma_1 = \sqrt{3}, \sigma_2 = 1$։
    
- **$\Sigma^+$ մատրից.** $\Sigma^+ = \begin{pmatrix} 1/\sqrt{3} & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$։
    

Քայլ 2. Հաշվել $A^+$

Բազմապատկելով $V \Sigma^+ U^T$ մատրիցները, ստանում ենք $A^+$ կեղծ հակադարձը։

$$A^+ = \begin{pmatrix} 1/3 & -1/3 & 2/3 \\ 1/3 & 2/3 & -1/3 \end{pmatrix}$$

Քայլ 3. Գտնել լավագույն մոտարկումը ($x_0$)

$x_0 = A^+ b = \begin{pmatrix} 1/3 & -1/3 & 2/3 \\ 1/3 & 2/3 & -1/3 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$։

- $x_{0_1} = 1/3 - 2/3 + 6/3 = 5/3$
    
- $x_{0_2} = 1/3 + 4/3 - 3/3 = 2/3$
    
    **$x_0 = \begin{pmatrix} 5/3 \\ 2/3 \end{pmatrix}$**։
    

Քայլ 4. Ստուգում և սխալի հաշվում

Հաշվում ենք $Ax_0$ արտադրյալը և համեմատում $b$-ի հետ․

$$Ax_0 = \begin{pmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 5/3 \\ 2/3 \end{pmatrix} = \begin{pmatrix} 7/3 \\ 2/3 \\ 5/3 \end{pmatrix}$$

Սխալը՝ $\|Ax_0 - b\|^2 = (7/3-1)^2 + (2/3-2)^2 + (5/3-3)^2 = \frac{16}{9} + \frac{16}{9} + \frac{16}{9} = \frac{48}{9} = \frac{16}{3}$։ Սա այն նվազագույն հնարավոր սխալն է, որը կարող ենք ստանալ։

---
> 
> **Ինչպես է $A^+$ մատրիցը օգնում լուծել Գծային Ռեգրեսիայի խնդիրները տվյալների գիտության մեջ:**
> 
> Գծային ռեգրեսիան տվյալների գիտության (Data Science) ամենահիմնարար գործիքներից է, և **կեղծ հակադարձ մատրիցը ($A^+$)** հանդիսանում է դրա մաթեմատիկական «սիրտը»։ Ահա թե ինչպես է այն աշխատում գործնականում.
> 
> ---
> 
> **1. Խնդրի էությունը. Տվյալների գերբեռնվածություն**
> 
> Պատկերացրեք՝ դուք ունեք տան գների վերաբերյալ տվյալներ (քառակուսի մետր, սենյակների քանակ) և ուզում եք կանխատեսել գինը։
> 
> - Դուք ունեք հազարավոր տվյալներ (տողեր), բայց ընդամենը մի քանի բնութագիր (սյուներ)։
>     
> - Սա ստեղծում է **գերորոշված համակարգ** ($m > n$), որտեղ հավասարումների քանակը շատ ավելին է, քան անհայտների քանակը:
>     
> 
> Նման դեպքում հնարավոր չէ գտնել մի գիծ, որը կանցնի **բոլոր** կետերով։ Մեզ պետք է գտնել այնպիսի գիծ, որը կանցնի կետերի «միջով»՝ նվազագույնի հասցնելով ընդհանուր սխալը։
> 
> ![Image of Linear regression least squares fit](https://encrypted-tbn3.gstatic.com/licensed-image?q=tbn:ANd9GcS9T_ckGdj3uwqzdP-nCDuJ7bvsxzroM-_uFYIBSkcOAxJntPFF0bZGiYmWRM3akNhEzgx7jDlfgxj1pzko65CrdYsf7v3n3jNPQKlp33jiAO2welk)
> 
> ---
> 
> **2. Ինչպե՞ս է $A^+$-ը լուծում սա**
> 
> Գծային ռեգրեսիայի խնդիրը ձևակերպվում է որպես $Ax = b$, որտեղ.
> 
> - **$A$**-ն ձեր տվյալների մատրիցն է (բնութագրերը):
>     
> - **$x$**-ը այն գործակիցներն են (weights), որոնք մենք ուզում ենք գտնել:
>     
> - **$b$**-ն թիրախային արժեքներն են (իրական գները):
>     
> 
> Քանի որ ճշգրիտ լուծում չկա, մենք օգտագործում ենք նախորդ քայլում քննարկված **լավագույն մոտարկման բանաձևը**.
> 
> $$x = A^+ b$$
> 
> Օգտագործելով SVD վերլուծությունը ($A^+ = V \Sigma^+ U^T$)՝ համակարգիչը վայրկյանական հաշվում է այն գործակիցները, որոնք **նվազագույնի են հասցնում սխալի քառակուսիների գումարը** ($\|Ax - b\| \to \min$):
> 
> ---
> 
> **3. $A^+$-ի առավելությունները ռեգրեսիայում**
> 
> Տվյալների գիտության մեջ $A^+$-ի օգտագործումը SVD-ի միջոցով ունի երկու հսկայական առավելություն.
> 
> - **Կայունություն (Numerical Stability).** Եթե ձեր տվյալների մեջ կան սյուներ, որոնք իրարից շատ ուժեղ կախված են (multicollinearity), սովորական մեթոդները ձախողվում են: Սակայն SVD-ն «տեսնում է» դա և շատ փոքր եզակի արժեքները ($\sigma$) զրոյացնելով՝ կանխում է սխալները։
>     
> - **Ունիվերսալություն.** Այն աշխատում է նույնիսկ եթե մատրիցը հակադարձելի չէ կամ ռանգը լիարժեք չէ։
>     
> 
> ---
> 
> **Ամփոփում. Ինչու՞ է սա կարևոր ձեզ համար**
> 
> Այն ամենը, ինչ մենք անցանք՝ **սեփական արժեքներ $\to$ SVD $\to$ Կեղծ հակադարձ**, տանում է դեպի սա.
> 
> > Երբ դուք համակարգչին ասում եք «կառուցիր ռեգրեսիայի մոդել», նա հետնաբեմում կատարում է հենց այս մաթեմատիկական գործողությունները՝ տվյալները պտտում է ($V^T$), ձգում է ($\Sigma$), նորից պտտում է ($U$) և վերջում տալիս է ձեզ օպտիմալ $x$ գործակիցները:
>